# -*- coding: utf-8 -*-
"""FGC_incorp_Y.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xIDnltOMzUGCU1eFDjL40I8VREGwfVRX
"""

# Commented out IPython magic to ensure Python compatibility.
#### FGC
!pip install deeprobust
# %load_ext autoreload
# %autoreload 2
# %matplotlib inline
from IPython.core.display import display, HTML
display(HTML("<style>.container { width:90% !important; }</style>"))
!pip install torch
from networkx.generators.random_graphs import erdos_renyi_graph
from networkx.generators.random_graphs import barabasi_albert_graph
from networkx.generators.community import stochastic_block_model
from networkx.generators.random_graphs import watts_strogatz_graph
from networkx.generators.community import random_partition_graph

import networkx as nx
import numpy as np
import matplotlib.pyplot as plt

import math
from tqdm import tqdm
import seaborn as sns
from sklearn.decomposition import FactorAnalysis

import sys
np.set_printoptions(threshold=sys.maxsize)

import random
from deeprobust.graph.data import Dataset
# Real Datasets Import
dataset_name = 'cora' #other datatsets : 'polblogs' , 'polblogs' , 'acm'
ori_nodes = 2708 
## citeseer : 3312
## cora     : 2708
## polblogs : 1490
## acm      : 3025
data = Dataset(root='', name=dataset_name, setting='gcn',seed=10)
#print(data.features)
adj, features, labels = data.adj, data.features, data.labels
idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test

print(idx_val.shape, idx_test.shape)

from sklearn.preprocessing import OneHotEncoder

Mask=np.zeros((idx_test.shape[0], ori_nodes))
for i in range(Mask.shape[0]):
  Mask[i][idx_test[i]]=1

idx_test=np.concatenate((idx_test, idx_val))
A = np.array(adj.todense())
X = np.array(features.todense())
onehot_encoder = OneHotEncoder(sparse=False)
integer_encoded = labels.reshape(len(labels), 1)
Y = onehot_encoder.fit_transform(integer_encoded)

Y_Train=Y.copy()
for i in range(2708):
  if ~(i in idx_test):
    vec=np.zeros(Y.shape[1])
    Y_Train[i]=np.zeros(Y.shape[1])
    for j in range(A.shape[1]):
      if(A[i][j]>0 and j in idx_test):
       
        vec=vec+A[i][j]*Y[j]
    
    Y_Train[i][np.argmax(vec)]=1

print(np.linalg.norm(Y_Train-Y)**2/2)

b=np.ones(ori_nodes)
D=np.diag(A@b)
L=D-A

import math
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import seaborn as sns
from sklearn.decomposition import FactorAnalysis
class solver_v2:

  def __init__(self, X, Y, k, M, lambda_param, beta_param, alpha_param, gamma_param, delta_param):
    self.X = X
    self.p = X.shape[0]
    self.k = k
    self.n = X.shape[1]
    self.Y=Y
    self.num_classes=Y.shape[1]
    self.M=M
    n = self.n
    k = self.k
    p = self.p
    c=self.num_classes

    self.thresh = 1e-10 # The 0-level

    
    self.C = np.random.normal(0,1,(p,k))
    self.C[self.C < self.thresh] = self.thresh

    self.Y_tilde= np.random.uniform(0,1,(k,c));

    # Model Hyperparameters
    self.beta_param = beta_param
    self.alpha_param = alpha_param
    self.lambda_param = lambda_param
    self.gamma_param = gamma_param
    self.delta_param=delta_param
    self.iters = 0
    self.lr0 = 1e-5

  def getLR(self):
    a = 0.99
    return self.lr0




  def calc_f(self):
    X_tilde = self.X_tilde
    beta_param = self.beta_param
    fw = 0
    J = np.outer(np.ones(self.k), np.ones(self.k))/self.k
    fw -= self.gamma_param*np.linalg.slogdet(self.C.T@L@self.C + J)[1]
    fw += (self.lambda_param)/2*((np.linalg.norm(np.dot(self.C, np.ones((self.k, 1)))))**2)
    fw+= (self.delta_param)/2*(np.linalg.norm(np.subtract(self.Y, self.M@self.C@self.Y_tilde)))**2
    return fw
  



  def grad_C(self):
        
    J = np.outer(np.ones(self.k), np.ones(self.k))/self.k
    v=np.linalg.pinv(self.C.T@L@self.C + J)
    gradC = np.zeros(self.C.shape)
    gradC += (self.lambda_param) * (np.abs(self.C) @ (np.ones((self.k, self.k))))
    gradC += -2*(self.gamma_param)*L@self.C@v
    gradC += self.delta_param*((self.C@self.Y_tilde - self.Y) @self.Y_tilde.T )
    
    return gradC

  def update_C(self, lr = None):
    if not lr:
      lr = 1/ (self.k)
    lr = self.getLR()
    C = self.C
    C = C - lr*self.grad_C()
    C[C<self.thresh] = self.thresh
    self.C = C
    C = self.C.copy()

    for i in range(len(C)):
      C[i] = C[i]/np.linalg.norm(C[i],1)

    self.C = C.copy()
    return None

  def update_Y_tilde(self):
    grad_Y_t = np.zeros(self.Y_tilde.shape)
    grad_Y_t+= np.linalg.pinv(self.C)@self.Y
    self.Y_tilde=grad_Y_t.copy()
    return None

  def fit(self, max_iters):
    ls = []
    MAX_ITER_INT = 100
    for i in tqdm(range(max_iters)):
      for _ in range(MAX_ITER_INT):
        self.update_C(1/self.k)
        self.update_Y_tilde()

      
      #ls.append(self.calc_f())
      self.iters+=1

    return (self.C, self.Y_tilde, ls ) 



  def set_experiment(self, X, X_t):
    self.X = X
    self.X_tilde = X_t

k = 30
overall_loss = []
overall_misclassification=[]
iterations = 10
obj = solver_v2(X, Y_Train, k, Mask, 500, 20, 650, 450, 900) 
C_0, Y_t_0, loss_ls = obj.fit(iterations)

def miscalss(C,Y_tilde,Y):
  '''
  for i in range(Y_tilde.shape[0]):
    j=np.argmax(Y_tilde[i])
    for k in range(Y_tilde.shape[1]):
      if(k!=j):
        Y_tilde[i][j]=0
      else:
        Y_tilde[i][j]=1
  '''

  Y_n=C@Y_tilde
  x=0
  for i in range(Y.shape[0]):
    if(np.argmax(Y[i])!=np.argmax(Y_n[i])):
      x=x+1
  return x
miscalss(C_0, Y_t_0, Y)

miscalss(C_0, Y_t_0, Y_Train)

def miscalss(C,Y_tilde,Y):
  
  for i in range(Y_tilde.shape[0]):
    j=np.argmax(Y_tilde[i])
    for k in range(Y_tilde.shape[1]):
      if(k!=j):
        Y_tilde[i][j]=0
      else:
        Y_tilde[i][j]=1
  

  Y_n=C@Y_tilde
  x=0
  for i in range(Y.shape[0]):
    if(np.argmax(Y[i])!=np.argmax(Y_n[i])):
      x=x+1
  return x
miscalss(C_0, Y_t_0, Y_Train)

eigen_values,eigenvectors=np.linalg.eig(L)

s=np.sort(eigen_values)

eigen_value,eigenvector=np.linalg.eig(C_0.T@L@C_0)

z=np.sort(eigen_value) 


s_new=s[-30:]
z_new=z[-30:]

temp=0
for j in range(len(s_new)):
  temp=temp+(abs(z_new[j]-s_new[j])/s_new[j])
eigenerror=temp/100
print(" eigen_error ")
print(eigenerror)

plt.plot(s_new, label="original")
plt.plot(z_new, label="coarsened")
plt.legend()
plt.show()

import seaborn as sns
import matplotlib.pylab as plt
# sns.heatmap(C_0.T@C_0)
a = sns.heatmap(C_0.T@C_0,cmap='CMRmap_r')
plt.title("Cora", x=0.5, y=0.9,weight="bold")
#a.figure.savefig("polblogs_heatmap_03.eps",dpi=1500)
